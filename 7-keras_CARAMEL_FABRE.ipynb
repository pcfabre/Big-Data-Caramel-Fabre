{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7-keras_CARAMEL_FABRE.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ClX7gdWGv7b7","colab_type":"text"},"source":["# Introduction\n","This practical work is about creation of elementary models in keras.\n","While it focuses on image classification, it illustrates many important functionalities of the keras framework and tries to provide the user with good deep-learning development strategies."]},{"cell_type":"markdown","metadata":{"id":"lidilM2ixDZK","colab_type":"text"},"source":["## Setup\n","Before you start, please mount your google drive.\n","For each exercise, create an output directory to store monitoring data and your model."]},{"cell_type":"code","metadata":{"id":"wzPwG_5Y9DN8","colab_type":"code","outputId":"773e0d38-e494-4099-9efc-efea6bfa561f","executionInfo":{"status":"ok","timestamp":1585734642793,"user_tz":-120,"elapsed":17685,"user":{"displayName":"Pauline Caramel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwspCiOrdO3BypYvFHd3CZayBhJsKDeVhm6iL-=s64","userId":"04031926914417928320"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YtL8KNsJxxGa","colab_type":"text"},"source":["# EX1: Lenet-like classifier."]},{"cell_type":"markdown","metadata":{"id":"5m0voN8YyLhD","colab_type":"text"},"source":["## 1- Build model architecture\n","The architecture has to be a CLASS called \"Lenet_like\".\n","\n","- I can define the width: number of filters in the first layer.\n","- I can define the depth: number of conv layers.\n","- I can specify the number of classes: output neurons.\n","\n","Lenet_like has no input tensor. But it has a \\_\\_call\\_\\_ function and can therefore be called on an input later.\n","\n","The rule to go from depth d to depth d+1, is to reduce the spatial size by a factor of 2 in each direction.\n","\n","Hidden dense layer will have 512 units."]},{"cell_type":"code","metadata":{"id":"NQ-tLkW79EWe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"976faf1c-d38c-4991-e14d-61a7a23b7442","executionInfo":{"status":"ok","timestamp":1585734652565,"user_tz":-120,"elapsed":2140,"user":{"displayName":"Pauline Caramel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwspCiOrdO3BypYvFHd3CZayBhJsKDeVhm6iL-=s64","userId":"04031926914417928320"}}},"source":["from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Dense\n","from keras.models import Model\n","\n","class Lenet_like:\n","  \n","  def __init__(self, width, depth, nb_classes):\n","    self.width = width\n","    self.depth = depth\n","    self.nb_classes = nb_classes\n","\n","  def __call__(self, input_tensor):\n","    y = Conv2D(self.width, kernel_size = (3,3), activation='relu', padding='same')(input_tensor)\n","    y = MaxPooling2D(pool_size=(2,2), padding='same')(y)\n","\n","    for d in range(self.depth) :\n","      y = Conv2D(self.width*2*(d+1), kernel_size = (3,3), activation='relu', padding='same')(y)\n","      y = MaxPooling2D(pool_size=(2,2), padding='same')(y)\n","    \n","    y = Dense(512, activation='relu')(y)\n","    out = Dense(self.nb_classes, activation='softmax')(y)\n","\n","    model = Model(inputs=input_tensor, outputs=out)\n","    model.compile(optimizer='adam',\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    return model"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"XMWL7DHzfIqv","colab_type":"code","colab":{}},"source":["# Test the class\n","test = Lenet_like(5,5,2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVKIV9Yhz1w2","colab_type":"text"},"source":["## 2- A function to create a model with Lenet_like architecture\n","I want the model to be able to fit on the following datasets:\n","\n","- mnist\n","- fashion-mnist\n","- cifar-10\n","\n","Create a function called \"make_lenet_model\" that take the name of one of these dataset as a str.\n","It returns a keras Model object.\n","It should obviously take all arguments to init the Lenet_like architecture.\n","Arguments other than the dataset might have default values.\n","I want to be able to monitor the accuracy of the model."]},{"cell_type":"code","metadata":{"id":"fYiDa77m9FJD","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from keras import datasets\n","\n","def make_lenet_model(name_dataset, width, depth, nb_classes):\n","\n","  if str(name_dataset) == 'mnist':\n","    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n","\n","  if str(name_dataset) == 'fashion-mnist':\n","    (x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n","  \n","  if str(name_dataset) == 'cifar-10':\n","    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n","\n","  model = Lenet_like(width, depth, nb_classes)\n","  input_tensor = len(x_train)\n","  out = model(input_tensor)\n","\n","  return out, x_train, y_train, x_test, y_test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HIJ9JH3x3VXU","colab_type":"text"},"source":["## 3- Create a fitting function\n","\n","I want a function \"fit_model_on\" with the following arguments:\n","\n","- dataset: str name of the dataset\n","- epochs: number of times you fit on the entire training set\n","- batch_size: number of images to average gradient on\n","\n","The function must create a Lenet model and fit it following these parameters.\n","The function should:\n","\n","- fit the model, obviously\n","- store the model architecture in a .json file in your output directory\n","- store the model's weights in a .h5 file in your output directory\n","- store the fitting metrics loss, validation_loss, accuracy, validation_accuracy under the form of a plot exported in a png file.\n","\n","Run your fitting function of course."]},{"cell_type":"code","metadata":{"id":"nl4UE4qc9Fzq","colab_type":"code","colab":{}},"source":["import json\n","from matplotlib import pyplot as plt\n","\n","path_output = '/drive/MyDrive/Colab Notebooks/Big Data/TP7/output/'\n","\n","def fit_model_on(dataset, epochs, batch_size, width, depth, nb_classes):\n","  # apply the function make_lenet_model\n","  model, x_train, y_train, x_test, y_test = make_lenet_model(dataset, width, depth, nb_classes)\n","  \n","  # fit the model\n","  model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n","\n","  # store the model architecture\n","  model_json = model.to_json()\n","  with open(path_output+'model.json', 'w') as json_file:\n","    json_file.write(model_json)\n","\n","  # store the model's weights\n","  model.save_weights(path_output+'model.h5')\n","\n","  #store the fitting metrics\n","  scores = model.evaluate(x_train, y_train, verbose=0)\n","  names_score = model.metrics_names\n","  plt.plot(score[2], label = name_score[2])\n","  plt.plot(score[3], label = name_score[3])\n","  plt.plot(score[4], label = name_score[4])\n","  plt.plot(score[5], label = name_score[5])\n","  plt.title(\"Metrics of the model on the train\")\n","  plt.legend(bbox_to_anchor=(1,1))\n","  plt.savefig('metrics.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NXzGYvqKEhQj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":579},"outputId":"431cd37a-ce7c-413a-de53-0db7f9b94d36","executionInfo":{"status":"error","timestamp":1585734671260,"user_tz":-120,"elapsed":12677,"user":{"displayName":"Pauline Caramel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwspCiOrdO3BypYvFHd3CZayBhJsKDeVhm6iL-=s64","userId":"04031926914417928320"}}},"source":["# Run the function\n","dataset = 'fashion-mnist'\n","epochs = 10\n","batch_size = 100\n","width = 10000\n","depth = 10\n","nb_classes = 2\n","\n","fit_model_on(dataset, epochs, batch_size, width, depth, nb_classes)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 5us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 2s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 1s 0us/step\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-11335f351924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnb_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfit_model_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-bbaacdf2fc44>\u001b[0m in \u001b[0;36mfit_model_on\u001b[0;34m(dataset, epochs, batch_size, width, depth, nb_classes)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_model_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# apply the function make_lenet_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_lenet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-2a6dc1d715c1>\u001b[0m in \u001b[0;36mmake_lenet_model\u001b[0;34m(name_dataset, width, depth, nb_classes)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLenet_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-c287e530277a>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2398\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2399\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=0. Full shape received: []","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: Current TensorFlow version is 2.2.0-rc2. To use TF 1.x instead,\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\nyou run \"import tensorflow\".\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"fl6kLBsxErmY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}